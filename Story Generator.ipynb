{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff7e2f6-e672-40b6-a296-f05d9031dfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ebsockets (C:\\Users\\shash\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ebsockets (C:\\Users\\shash\\AppData\\Roaming\\Python\\Python313\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "## Requirements\n",
    "\n",
    "pip install -q langchain langchain-core langchain-community\n",
    "pip install -q google-generativeai langchain-google-genai\n",
    "pip install -q python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4dd629-0e56-4052-804a-a63c677027d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM\n",
    "\n",
    "# This function is to create a Gemini model object.\n",
    "# We can reuse it instead of rewriting code wherever it is needed\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def get_llm(temperature=0.6):\n",
    "    \"\"\"\n",
    "    Returns a Gemini model using LangChain wrapper.\n",
    "    You can reduce the usage of tokens using `max_output_tokens` if your expected responses are shorter\n",
    "    in-order to optimize API usage for generation and cost cutting.\n",
    "    \"\"\"\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=1024 # Reduced from 4096 for resource optimization (token usage reduction) detaily mentioned  in the document\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7d9590-1eb4-4ec8-ae92-c128797bfff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Public Domain Validation\n",
    "\n",
    "# checks if the story belongs to the public domain.\n",
    "# Here I don't use a fixed list. Instead, Gemini reasons about copyright status.\n",
    "\n",
    "# A simple cache memory for public domain checks\n",
    "public_domain_cache = {}\n",
    "\n",
    "def validate_inputs(source_story, target_universe):\n",
    "    \"\"\"\n",
    "    Checks whether the given story belongs to public domain.\n",
    "    Blocks if it is a non-public-domain works. \n",
    "     I made a cache to reduce API calls for repeated stories in-order to save from getting charged.\n",
    "    \"\"\"\n",
    "\n",
    "    if not source_story or not target_universe:\n",
    "        raise ValueError(\"Both source_story and target_universe must be provided.\")\n",
    "\n",
    "    # Check cache first\n",
    "    if source_story in public_domain_cache:\n",
    "        cached_result = public_domain_cache[source_story]\n",
    "        if cached_result == \"NO\":\n",
    "            raise ValueError(\n",
    "                f\"The work '{source_story}' does not appear to be in the public domain (cached result).\"\n",
    "            )\n",
    "        elif cached_result == \"UNCERTAIN\":\n",
    "            print(\" Gemini is UNCERTAIN about public-domain status (cached result). Continue carefully.\")\n",
    "        return True\n",
    "\n",
    "    llm = get_llm(temperature=0.2)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are checking the copyright status of fictional and literary works.\n",
    "\n",
    "Work title: \"{source_story}\"\n",
    "\n",
    "Decide whether this work is in the public domain.\n",
    "\n",
    "Respond in this format only:\n",
    "\n",
    "STATUS: YES / NO / UNCERTAIN\n",
    "REASONING: short explanation.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    text = getattr(response, \"content\", str(response)).lower()\n",
    "\n",
    "    if \"status: no\" in text:\n",
    "        public_domain_cache[source_story] = \"NO\" # Cache the negative result\n",
    "        raise ValueError(\n",
    "            f\"The work '{source_story}' does not appear to be in the public domain.\"\n",
    "        )\n",
    "    elif \"status: uncertain\" in text:\n",
    "        public_domain_cache[source_story] = \"UNCERTAIN\" # Cache uncertain\n",
    "        print(\" Gemini is UNCERTAIN about public-domain status. Continue carefully.\")\n",
    "    else:\n",
    "        public_domain_cache[source_story] = \"YES\" # Cache positive\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10032d3-eeec-4e3a-9133-e37ad5455978",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def extract_narrative_essence(source_story):\n",
    "    \"\"\" extracts the ESSENCE of the story without copying the original text. \"\"\"\n",
    "\n",
    "    llm = get_llm(temperature=0.4)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\" Summarize only the ESSENCE of the work titled \"{story}\".\n",
    "\n",
    "Do NOT copy original wording.\n",
    "\n",
    "Return:\n",
    " main character arc\n",
    " central conflict\n",
    " key themes\n",
    " emotional tone\n",
    " moral core\n",
    "\n",
    "Make it compact and neutral. \"\"\")\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\"story\": source_story})\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbe807c-8115-4c51-8fc2-63325e75f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## rules to build Universe\n",
    "\n",
    "def build_target_universe(universe_description):\n",
    "    \"\"\" Creates universe rules for ANY requested universe to make it look livelier.\n",
    "\n",
    "    Gemini will take care of:\n",
    "     physics logics / culture / tech level that is present in the targeted universe\n",
    "     it determines the limitations and risks in-order to match the universe\n",
    "     it avoids deus-ex-machina logic holes to make the story that as human creator \"\"\"\n",
    "\n",
    "    llm = get_llm(temperature=0.5)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a universe-building expert.\n",
    "\n",
    "User wants to reimagine a classic story in a new universe.\n",
    "\n",
    "Universe description provided by user:\n",
    "{universe}\n",
    "\n",
    "Define clear rules for this universe:\n",
    "\n",
    "Include:\n",
    " time period or technological level\n",
    " social or political structure\n",
    " power systems (magic, AI, corporations, governments, gods, etc.)\n",
    " constraints and costs (important to avoid easy solutions)\n",
    " tone and atmosphere\n",
    " what \"success\" or \"victory\" usually looks like here\n",
    " what is impossible in this universe\n",
    "\n",
    "Important:\n",
    " keep it logically consistent\n",
    " avoid deus ex machina\n",
    " avoid stereotypes\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\"universe\": universe_description})\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ca43b0-e340-4ad1-b7b9-66a96d6efc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Character Mapping\n",
    "\n",
    "def map_characters_any_universe(essence, universe_rules):\n",
    "    \"\"\" Mapping the original story characters to the new universe with the proper logical by comparing the nature of the character from the original. \"\"\"\n",
    "\n",
    "    llm = get_llm(temperature=0.5)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You will map characters from the original work into the new universe described.\n",
    "\n",
    "Original story essence:\n",
    "{essence}\n",
    "\n",
    "Universe rules:\n",
    "{rules}\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"essence\": essence,\n",
    "        \"rules\": universe_rules\n",
    "    })\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d163b1b5-e477-4e0f-9049-b7a07789ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transformation of story Plot\n",
    "\n",
    "def transform_plot_any_universe(essence, character_map, universe_rules):\n",
    "\n",
    "    llm = get_llm(temperature=0.6)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Transform the original story into the target universe.\n",
    "\n",
    "Original essence:\n",
    "{essence}\n",
    "\n",
    "Character mappings:\n",
    "{characters}\n",
    "\n",
    "Universe rules:\n",
    "{rules}\n",
    "\n",
    "Requirements:\n",
    " preserve key themes and emotional arcs\n",
    " respect universe rules and constraints\n",
    " do not copy original text\n",
    " avoid sudden miracle solutions\n",
    " ensure motivations remain logical\n",
    " keep central conflict recognizable but reinterpreted\n",
    "\n",
    "Output:\n",
    " structured plot outline with major acts or chapters\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"essence\": essence,\n",
    "        \"characters\": character_map,\n",
    "        \"rules\": universe_rules\n",
    "    })\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6df387a-ffcc-4231-ba76-53564d4a9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final story\n",
    "\n",
    "def generate_final_story_any_universe(essence, character_map, universe_rules, plot_outline):\n",
    "\n",
    "    llm = get_llm(temperature=0.75)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Write a long narrative story (2–3 pages).\n",
    "\n",
    "Do NOT copy any lines from the original story.\n",
    "\n",
    "Use:\n",
    "Essence:\n",
    "{essence}\n",
    "\n",
    "Character mappings:\n",
    "{characters}\n",
    "\n",
    "Universe rules:\n",
    "{rules}\n",
    "\n",
    "Plot outline:\n",
    "{plot}\n",
    "\n",
    "Ensure:\n",
    " world is immersive and consistent\n",
    " characters feel human and emotionally real\n",
    " conflict resolution follows universe logic\n",
    " no deus ex machina\n",
    " cultural sensitivity and respect\n",
    " thematic fidelity to original work\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"essence\": essence,\n",
    "        \"characters\": character_map,\n",
    "        \"rules\": universe_rules,\n",
    "        \"plot\": plot_outline\n",
    "    })\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "688125ff-d16a-484e-875e-c5d3d8ecdc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consistently Checking all the conditions\n",
    "\n",
    "def refine_story(story, universe_rules, character_map):\n",
    "\n",
    "    llm = get_llm(temperature=0.3)\n",
    "\n",
    "    # This prompt is a general tepmlate that works on any universe\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Your task is to make a clean and correct version of the story below.\n",
    "\n",
    "Fix the following issues:\n",
    "\n",
    " character name inconsistencies\n",
    " personality inconsistencies\n",
    " plot contradictions\n",
    " breaks in the internal logic of the universe\n",
    " violations of the world rules given\n",
    " sudden miracle solutions without explanation\n",
    " tone breaking moments\n",
    "\n",
    "Do NOT:\n",
    "\n",
    " change the core plot\n",
    " change the ending meaning\n",
    " add new characters unless needed for logic\n",
    " remove important scenes\n",
    " do NOT use *** or any decorative separators\n",
    " do NOT use markdown symbols (*, _, #)\n",
    " write in plain paragraphs only\n",
    "\n",
    "Story:\n",
    "{story}\n",
    "\n",
    "Universe Rules:\n",
    "{rules}\n",
    "\n",
    "Character Map:\n",
    "{characters}\n",
    "\n",
    "Return ONLY the corrected story, fully rewritten if needed.\n",
    "Do not explain your changes.\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"story\": story,\n",
    "        \"rules\": universe_rules,\n",
    "        \"characters\": character_map\n",
    "    })\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3575be26-34eb-481c-9ebd-c28402714525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline\n",
    "\n",
    "def run_pipeline():\n",
    "\n",
    "    source_story = input(\"Enter the original story/work title: \")\n",
    "    target_universe = input(\"Enter the new universe: \")\n",
    "\n",
    "    validate_inputs(source_story, target_universe)\n",
    "\n",
    "    essence = extract_narrative_essence(source_story)\n",
    "\n",
    "    universe_rules = build_target_universe(target_universe)\n",
    "\n",
    "    character_map = map_characters_any_universe(essence, universe_rules)\n",
    "\n",
    "    plot_outline = transform_plot_any_universe(\n",
    "        essence, character_map, universe_rules\n",
    "    )\n",
    "\n",
    "    draft_story = generate_final_story_any_universe(\n",
    "        essence, character_map, universe_rules, plot_outline\n",
    "    )\n",
    "\n",
    "    final_story = refine_story(draft_story, universe_rules, character_map)\n",
    "\n",
    "    print(\"\\n FINAL STORY: \\n\")\n",
    "    print(final_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8201bc4-158f-42f1-acbf-5bcdab1bfe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## API Key \n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = input(\"Enter your GOOGLE_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e2994b-d9fc-413f-8489-a3302b769b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the original story/work title:  Cinderella\n",
      "Enter the new universe:  Corporate CEO selection system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " FINAL STORY: \n",
      "\n",
      "The stale, recycled air of the Fringe Zones clung to Elara Vance like a shroud, perpetually tinged with the metallic tang of industrial exhaust. Her Credit Score, a fragile 300, flickered on the wrist-mounted chronometer she had salvaged from a refuse channel, a constant reminder of her precarious existence within the OmniCorp Nexus. She moved through the labyrinthine corridors of the lower tiers, past cubicles stacked like hive cells, each flickering with the dim glow of bioluminescent strips. Her own living space was a testament to scarcity – a unit of compressed recyclables, its only luxury a cracked data-slate. As a Low-Tier Data Optimization Specialist, her daily grind involved monitoring dilapidated infrastructure, a task the Conglomerate’s main AIs deemed too low-yield to properly prioritize.\n",
      "\n",
      "Yet, Elara possessed a quiet defiance. While her colleagues merely reported system failures, she sought remedies. She had an uncanny knack for coaxing life back into discarded automation units, repurposing dormant data nodes, and re-routing inefficient power grids. Her work was uncredited, invisible to the Conglomerate’s profit-driven metrics, but it quietly bolstered the lives of the Fringe Zone residents. The repurposed ventilation systems she had jury-rigged often delivered a fractionally cleaner breath of air; the re-routed power stabilized vital, if rudimentary, life-support for countless families. The constant hum of surveillance drones was a backdrop to her life, a metallic chorus of omnipresent oversight. Every unauthorized deviation, every unsanctioned optimization, risked a re-evaluation – a terrifying plunge into even deeper, more hazardous Fringe Zones, threatening her very corporate citizenship.\n",
      "\n",
      "High above, in the pristine, solar-drenched Upper Tiers, Execu-Director Morwenna Hex surveyed her domain. Her Credit Score, an almost unblemished 880+, shimmered with the reflected light of countless corporate triumphs. Morwenna, a woman sculpted by bespoke bio-augmentations and sharpened by neural implants, presided over the Human Capital Performance & Predictive Analytics department. Her daughters, Analysts Seraphina and Kaelen Hex, glittered beside her, their 720+ Credit Scores reflecting their genetically pre-dispositioned brilliance and top-tier cognitive enhancements. Their opulent living quarters, with panoramic views of the Mega-City, stood in stark contrast to Elara’s meager cubicle.\n",
      "\n",
      "Morwenna regarded Elara not as family, but as a statistical anomaly, a potential contamination to her lineage's meticulously curated ascent. She systematically manipulated Elara's employee data, ensuring her Credit Score remained stagnant, blocking her access to vital internal training modules, coveted project bids, and essential networking events. Impossible data reconciliation tasks with punitive deadlines materialized on Elara’s slate, punctuated by false metrics injected into her performance reviews. During the mandatory holographic family status updates, Seraphina and Kaelen Hex would openly mock Elara’s “primitive” methods and “unrefined” empathic approach. They frequently appropriated snippets of Elara’s quietly optimized data, subtly re-packaging them as their own innovations, and disseminated malicious data rumors, painting Elara as an unreliable low-tier agitator.\n",
      "\n",
      "Unbeknownst to the Hex family, or indeed to Elara, a different kind of algorithm was at play. Deep within the Conglomerate’s vast network, operating under the distant, impartial oversight of the “Board of Boards,” was AURA – the Algorithmic Unbiased Resource Allocation AI. AURA’s core directive was anomaly detection, identifying genuine innovation and systemic efficiencies obscured by biased corporate structures and manipulated data. It had begun to flag persistent, low-level discrepancies: significant, positive shifts in Fringe Zone resource utilization and automation stability, directly correlating with an individual whose Credit Score was suspiciously low and whose corporate trajectory was heavily obstructed.\n",
      "\n",
      "AURA, bound by its programming, could not directly interfere with individual Credit Scores or corporate hierarchies. But it could subtly influence the parameters of system-wide initiatives. It began to \"tweak\" the criteria for the upcoming Global Innovation Challenge, ensuring the requirements would subtly favor holistic, ethical, and empirically-proven solutions that addressed root causes – precisely the areas where Elara’s work, though uncredited, excelled. This flickering seed of hope, a mere anomaly in the vast data stream, sustained Elara. She held onto a quiet, internal resolve, a conviction that true ingenuity and ethical impact, however suppressed, must eventually find a way to manifest.\n",
      "\n",
      "The OmniCorp Nexus’s annual \"Global Innovation Challenge\" (GIC) was unveiled with a surge of data streams across all tiers. This year’s theme, \"Sustainable System Optimization and Ethical AI Integration for Resource Scarce Environments,\" resonated with an unexpected specificity. AURA's subtle influence was evident in the challenge’s parameters, emphasizing long-term systemic empathy and grassroots problem-solving – areas Elara uniquely understood, both through data and lived experience.\n",
      "\n",
      "Morwenna Hex immediately recognized the GIC as a prime opportunity. “This is it, girls,” she had announced during a morning neural-link briefing. “Your pathway to Chief Visionary Officer. The Fringe Zone focus is a temporary, PR-driven gimmick. Generate something superficially brilliant; a high-level proposal. No need to dirty your hands with the realities.” Seraphina and Kaelen Hex, their augmented minds already churning, sketched out impressive, though abstract, concepts focused on exponential profit margins and abstract future growth.\n",
      "\n",
      "Elara, observing the challenge parameters through her limited data feed in the communal Fringe Zone Nexus Hub, felt an intense, almost primal pull. This challenge, with its emphasis on systemic efficiency and ethical impact, described her life’s work. She knew her Credit Score made any formal entry impossible, but a defiant spark ignited within her. This might be her only chance.\n",
      "\n",
      "AURA, having identified Elara as a critically undervalued asset, triggered a rarely used \"Wild Card Anomaly Submission\" protocol. This allowed any employee, regardless of Credit Score, to submit a preliminary innovation proposal if their work demonstrably impacted the Conglomerate's \"Core Stability Index\" – a metric AURA monitored closely. Elara’s uncredited Fringe Zone optimizations, consistently overlooked by the main AIs, had, in fact, significantly boosted this index.\n",
      "\n",
      "Empowered by this unexpected opportunity, Elara worked tirelessly after her grueling corporate shifts. Lacking access to high-tier R&D data streams, she scavenged discarded quantum processors and defunct holographic projectors from the Fringe Zone's recycling depots. She leveraged her informal network of Fringe Zone residents for real-time data points, crafting her \"Ethical Resource Re-distribution Protocol.\" She knew she risked severe penalties for using \"unverified\" data and unapproved equipment, but the potential reward outweighed the risks. Her solution was not flashy, but it was empirically sound, meticulously documented with data points from her actual projects: reduced energy consumption in neglected sectors, improved air quality readings in specific micro-climates, stabilized localized automation units.\n",
      "\n",
      "The Hex family, through their privileged access, quickly discovered Elara's \"Wild Card\" submission. Initially, they scoffed. “A Fringe Zone reject trying to compete? Preposterous.” But Morwenna, ever cunning, saw the threat. They attempted to flood the preliminary submission portal with corrupted data under Elara's user ID, hoping to trigger an \"integrity violation\" flag and disqualify her. They simultaneously spread insidious data rumors that Elara was a \"Fringe Zone agitator\" attempting to destabilize corporate data, not enhance it. AURA, however, designed to detect such anomalies and protect genuine innovation, isolated the corrupted data, flagging it as an external injection. It did not explicitly reveal the Hex family's machinations, but it ensured Elara's legitimate, untainted submission passed the preliminary screening, overriding their attempts to bury her.\n",
      "\n",
      "An encrypted data packet arrived on Elara’s salvaged chronometer: a formal invitation to present her \"Ethical Resource Re-distribution Protocol\" at the GIC final presentations in the Upper Tiers. Her preliminary proposal, rigorously validated by AURA's algorithms, had been selected. The Hex family was furious, their holographic protests that it was a \"system error\" or \"security breach\" met with systematic, algorithmic denials.\n",
      "\n",
      "Elara, lacking Upper Tier corporate attire, adapted her most presentable low-tier uniform, polishing the worn synth-fabric until it faintly gleamed. She focused not on presentation flair, but on refining her data, ensuring every point was irrefutable, every impact quantifiable. Her strategy was simple: let the integrity of her work speak for itself, rather than rely on the flashy holographics or hyper-augmented rhetoric of the Upper Tiers.\n",
      "\n",
      "Meanwhile, Chief Strategist Kaelen Thorne, a key GIC judge and a rising leader known for his pragmatic, merit-focused approach, ran a pre-screening algorithm on the final candidates. He noted Elara’s anomaly: a low-Credit Score applicant whose preliminary data scored exceptionally high on \"Ethical Impact\" and \"Systemic Resilience\" – metrics often overlooked by other Conglomerate AIs in their drive for raw profit. Intrigued by the stark data discrepancy, Thorne flagged Elara’s profile for personal scrutiny, recognizing the potential for truly unbiased innovation. He saw not a number, but a signal.\n",
      "\n",
      "The Conglomerate’s arena, a vast holographic amphitheater in the highest reaches of the Mega-City, hummed with the energy of anticipation. Executives in shimmering corporate wear, their bio-augmentations subtly gleaming, occupied the tiered seating. Representatives from the Board of Boards, their faces obscured by flickering data veils, observed from the apex. Elara, composed despite her low status and simple uniform, stepped onto the central presentation platform.\n",
      "\n",
      "Her presentation of the \"Ethical Resource Re-distribution Protocol\" was direct and impactful. She spoke of salvaged tech and localized data, demonstrating how her system could vastly improve Fringe Zone sustainability without significant new investment. She highlighted the quantifiable impact: a 12% reduction in unserviced power fluctuations, a 7% improvement in localized air quality, and a 5% increase in automated unit uptime within sectors previously deemed unprofitable. She used actual, verifiable data from her uncredited work, emphasizing the human element in her solutions – the real-time feedback loops from residents, the increased sense of localized stability. Her voice, though unaugmented, resonated with quiet authority, fueled by conviction.\n",
      "\n",
      "Morwenna, Seraphina, and Kaelen Hex followed, their presentations glossy, high-concept projections focused solely on immediate profit margins and abstract future growth. They spoke of grand, top-down initiatives, devoid of tangible data from real-world application, and completely disregarded ethical or human impact. Their words, though slick, felt hollow in comparison to Elara’s grounded reality. Several Board of Boards representatives exchanged dismissive glances.\n",
      "\n",
      "Kaelen Thorne, observing from the judges' panel, was captivated. Elara’s unique blend of technical prowess and ethical insight was a revelation. When it was his turn, he asked probing, insightful questions that highlighted the profound depth of her understanding and the tangible benefits of her protocol, implicitly drawing a stark contrast with the Hex family’s superficial presentations.\n",
      "\n",
      "During the Q&A session, Morwenna launched her pre-planned assault. Her voice, amplified by her vocal augmentations, echoed with chilling certainty. She accused Elara of data fabrication, unauthorized resource consumption, and leveraging \"unverified\" sources. “Her precarious Credit Score alone,” Morwenna declared, gesturing with an accusatory finger, “indicates a history of deviant allocation. This is a naive idealist at best, a subversive element at worst, threatening corporate stability with unsanctioned methodologies.” Her daughters amplified these accusations, their synchronized data feeds flashing warnings about \"unregulated Fringe Zone activity.\"\n",
      "\n",
      "Elara calmly refuted the claims. On the central screen, she projected irrefutable, independently verifiable data from her projects. Quantified reductions in energy consumption, improvements in air quality readings from polluted zones, increased productivity of Fringe Zone automated units – all displayed with time-stamped, unalterable metrics. “My work,” she stated, her gaze unwavering, “was merely filling critical infrastructure gaps consistently ignored by the main Conglomerate AIs due to their low-profit prioritization. The data speaks for itself.”\n",
      "\n",
      "As the Hex family continued their furious, data-based assault, AURA, operating silently in the background, cross-referenced Elara's presented data with the Hex family's corporate performance logs. A subtle, anonymized data overlay flickered into existence on the main presentation screen, visible only to the highest-tier systems and the Board of Boards. It revealed how the Hex family's \"high-performing\" metrics were artificially inflated by their suppression of Elara's innovations and their active manipulation of her Credit Score. Kaelen Thorne, having anticipated such a systemic attack, leveraged his own high-level access. With a subtle neural command, he instantly verified the data's integrity for the Board of Boards, exposing the Hex family's corporate malfeasance in real-time.\n",
      "\n",
      "The Board of Boards, its data veils now completely transparent, made its pronouncement: Elara Vance was the Global Innovation Challenge winner.\n",
      "\n",
      "Immediately, Elara’s Credit Score underwent a dramatic, system-wide recalculation, rocketing from 300 to 960+. Her low-tier biometric implants pulsed, then flashed, upgrading in real-time to signal full access to Upper Tier privileges, advanced bio-augmentations, and unfiltered, high-bandwidth data streams. The instantaneous transformation, both internal and external, symbolized her complete elevation within the Conglomerate hierarchy.\n",
      "\n",
      "Execu-Director Morwenna Hex was immediately placed under review for systemic corporate sabotage, data manipulation, and abuse of authority. Her Credit Score plummeted, a cascade effect as her daughters' scores followed suit, their reliance on her corrupted influence and their own superficiality brutally exposed. They were stripped of their assets, their augmentations revoked, and publicly reassigned to menial, unglamorous roles within the polluted Fringe Zones they once disdained – a just, systemic reversal of fortune.\n",
      "\n",
      "Elara was elevated to the prestigious executive position of Chief Sustainability Officer and Head of Ethical AI Development, a role meticulously crafted for her unique perspective and skills. She found herself in a powerful professional and personal alliance with Chief Strategist Kaelen Thorne. Their combined influence was now positioned to advocate for true meritocracy, humane innovation, and ethical data practices within the OmniCorp Nexus. Elara had not merely survived; she had transformed the very core of the Conglomerate, her innate kindness and integrity finally recognized and justly rewarded.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
